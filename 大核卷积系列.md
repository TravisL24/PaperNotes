# 1. Large-Kernel Attention for 3D Medical Image Segmentation

**个人体会**：

    1.LKA用在了上采样部分是一个全新的思考，这样的作用是什么?

摘要：提出了一个全新的大核卷积模块，专门用来**处理器官和肿瘤分割问题**。

            大核卷积模块 分解成小卷积去**降低计算量**，并且**便于集成**

主要贡献：

        1.  提出了 分解的LK conv，结合了conv和自注意力的优点并避免了缺点，

        2.  **自适应**放大关键特征的权重，同时降低噪声体素的权重（<u>怎么自适应</u>？）

## 主要模块

### 1.LK Attention

![lk attention.png](https://raw.githubusercontent.com/TravisL24/pic-repo/main/picGo/2022/11/03/20221103142512.png)

LK模块被拆解为了 5 5 5 DW conv + 7 7 7 DWD Conv + 1 1 1 Conv

conv前使用了 GN + leaky Relu

conv后用Sigmoid激活

**思想**：利用大核卷积去实现conv和自注意力的双重优势结合（利用demonstrate 去减少计算量）

**推理过程**：

（1）（2）代表<u>**K**大小卷积核的 parameter 和flops</u>

![微信截图_20221103155659.png](https://raw.githubusercontent.com/TravisL24/pic-repo/main/picGo/2022/11/03/20221103155707.png)

    （3）（4）代表分解后的

![微信截图_20221103155913.png](https://raw.githubusercontent.com/TravisL24/pic-repo/main/picGo/2022/11/03/20221103155921.png)

    

  对（3）求导，求取最优的d

![微信截图_20221103162413.png](https://raw.githubusercontent.com/TravisL24/pic-repo/main/picGo/2022/11/03/20221103162444.png)

通过实验展示出了随着C的扩大，decompose的效果越好

![微信截图_20221105152022.png](https://raw.githubusercontent.com/TravisL24/pic-repo/main/picGo/2022/11/05/20221105152034.png)

### 2. 网络模块

![屏幕截图 2022-11-05 154021.png](https://raw.githubusercontent.com/TravisL24/pic-repo/main/picGo/2022/11/05/20221105154033.png)

## 实验设计

### 1.数据采集

      **BraTs**：四个模态 到同一个模板 240 * 240 * 155，然后进行1 $mm^3 $插值 + 头骨剥离

       **CT-ORG** ：21个测试的分割是自己手动在itk-snap下分割的

### 2.预处理和DA

    **BraTS**：160 * 192 * 128 输入，intensity normalization做预处理

    **CT-ORG**：128 * 128 * 256 作为输入，volumes 重采样为 3 $mm^3$ 。重采样是 Gaussian smoothing + 分辨率插值

![屏幕截图 2022-11-05 161543.png](https://raw.githubusercontent.com/TravisL24/pic-repo/main/picGo/2022/11/05/20221105161552.png)

### 3.训练、优化、评估

        **BraTS**：BCE + soft Dice loss

        **CT-ORG**：weighted soft Dice loss

        Metric： Dice + HD95

### 4. 结果

    **消融实验**：

        1.LK在不同层级都有效果

        2.decomposed的和original的差距不大

        3.decomposed的参数降低非常多

![屏幕截图 2022-11-05 234338.png](https://raw.githubusercontent.com/TravisL24/pic-repo/main/picGo/2022/11/05/20221105234348.png)

        4.相同位置 kernel越大，效果越好

        5.相同的kernel，在中间层效果最好

![屏幕截图 2022-11-05 234838.png](https://raw.githubusercontent.com/TravisL24/pic-repo/main/picGo/2022/11/05/20221105234847.png)

**对比实验：**

![屏幕截图 2022-11-05 235044.png](https://raw.githubusercontent.com/TravisL24/pic-repo/main/picGo/2022/11/05/20221105235053.png)

**做了the paired t-test**

![屏幕截图 2022-11-05 235422.png](https://raw.githubusercontent.com/TravisL24/pic-repo/main/picGo/2022/11/05/20221105235445.png)

# 2. 3D UX-Net

## 摘要：

    1.Transformer和conv的混合方法**归功于**non-local 自注意力的大感受野 + 大量的模型参数

    2.本文提出了一个 轻量的volumetric convnet 

    3.本文的conv<u> 尝试使用7 * 7 * 7以上的大核卷积</u>（为了更大的感受野）
    4.用 <u>pointwise depth conv 替换掉Transformer里的MLP块</u>

## 相关工作

    1.VITs 在3D中展示了它们的能力，可以**提取带有体积patch的顺序表示**，并生成非局部自注意对应，以增强与体积相关的下游任务，特别是在医学图像分割方面。

    2. SwimUNETR 使用Swim Transformer作为backbones，并且现在是volumetric seg SOTA。  

    3.本模型的点：大核卷积来扩大感受野 + pointwise depth conv扩展到 更广的hidden dim，减少channels的冗余。

 **对比**：

![屏幕截图 2022-11-06 222839.png](https://raw.githubusercontent.com/TravisL24/pic-repo/main/picGo/2022/11/06/20221106222910.png)

    4. 3D $U^2$-Net 利用共享跨通道信息的 depth-wise convNet实现很好的效果，但只要1%参数

## 模型部分

**Encoder部分**，作者从block-wise和layer-wise两个方面思考

**block-wise部分**：

    1. patch-wise features projection：用了一个<u>大核projection层来提取patch-wise的特征</u>作为encoder的输入。

    2.大核的Volumetric depth-wise conv：用per-channel的思想来替代MSA

    3.inverted bottleneck：扩大四倍的channel，并用了 DCS 独立的线性放缩channel

**Layer-wise部分**：

    残差（sum前后都没有归一化和激活） + LN + GELU

## 实验部分

![屏幕截图 2022-11-08 220612.png](https://raw.githubusercontent.com/TravisL24/pic-repo/main/picGo/2022/11/08/20221108220713.png)
